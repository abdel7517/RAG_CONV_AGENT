# =============================================================================
# RAG CONVERSATIONAL AGENT - Environment Configuration
# =============================================================================
# Copy this file to .env and fill in your values
# For Docker: Most values are auto-configured via docker-compose.yml
# =============================================================================

# === PROVIDER LLM ===
# Choisir entre "ollama" (local), "mistral" (API cloud) ou "openai" (API cloud)
LLM_PROVIDER=ollama

# Temperature du modele (0.0 = deterministe, 1.0 = creatif)
MODEL_TEMPERATURE=0.7

# === PROVIDER EMBEDDINGS (optionnel) ===
# Permet d'utiliser un modele d'embedding d'un provider different du LLM
# Valeurs possibles: ollama, mistral, openai, huggingface
# Si non defini, utilise la valeur de LLM_PROVIDER
# EMBEDDING_PROVIDER=huggingface

# === CONFIGURATION OLLAMA (si LLM_PROVIDER=ollama) ===
OLLAMA_MODEL=phi3:mini
# Local: http://localhost:11434 | Docker: http://ollama:11434
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# === CONFIGURATION MISTRAL (si LLM_PROVIDER=mistral) ===
# Obtenez votre cle API sur: https://console.mistral.ai/
MISTRAL_API_KEY=votre_cle_api_mistral
MISTRAL_MODEL=mistral-small-latest
MISTRAL_EMBEDDING_MODEL=mistral-embed

# === CONFIGURATION OPENAI (si LLM_PROVIDER=openai) ===
# Obtenez votre cle API sur: https://platform.openai.com/api-keys
OPENAI_API_KEY=votre_cle_api_openai
OPENAI_MODEL=gpt-4o-mini
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# === CONFIGURATION HUGGINGFACE EMBEDDINGS (si EMBEDDING_PROVIDER=huggingface) ===
# Modeles locaux via sentence-transformers, pas besoin de cle API
HUGGINGFACE_EMBEDDING_MODEL=intfloat/multilingual-e5-large

# === CONFIGURATION GOOGLE CLOUD STORAGE (upload de documents PDF) ===
# Nom du bucket GCS (obligatoire pour l'upload)
GCS_BUCKET_NAME=votre-bucket-name
# ID du projet GCP
GCS_PROJECT_ID=votre-project-id
# Cle JSON du service account (coller le contenu du fichier JSON)
GCS_SERVICE_ACCOUNT_KEY={}
# Taille max upload (default: 10MB)
# MAX_UPLOAD_SIZE_BYTES=10485760
# Pages max par company (default: 5)
# MAX_PAGES_PER_COMPANY=5

# === CONFIGURATION REDIS (API + Worker) ===
# Local: redis://localhost:6379 | Docker: redis://redis:6379
REDIS_URL=redis://localhost:6379

# === CONFIGURATION POSTGRESQL (memoire long-terme + pgvector) ===
# Local: localhost | Docker: postgres (service name)
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=agent_memory
POSTGRES_USER=postgres
POSTGRES_PASSWORD=votre_mot_de_passe

# Alternative: Connection string complete (prioritaire si definie)
# DATABASE_URL=postgresql://user:password@host:port/database

# === RAG CONFIGURATION ===
PGVECTOR_COLLECTION_NAME=documents
DOCUMENTS_PATH=./documents
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
RETRIEVER_K=3

# === SYSTEM PROMPTS ===
SYSTEM_PROMPT=Vous etes un agent de service client francais professionnel et courtois. Votre role est d'aider les clients avec leurs questions et preoccupations. Soyez toujours poli, empathique et oriente solution. Maintenez le contexte de la conversation et fournissez des reponses claires et concises en francais.

# Optionnel: Prompt systeme specifique pour l'agent RAG
# SYSTEM_PROMPT_RAG=...

# === LANGSMITH (Optional Tracing) ===
# LANGCHAIN_TRACING_V2=true
# LANGCHAIN_API_KEY=votre-cle-langsmith
